# Getting your data

## Download

Move into your data folder in your main folder with `cd`.
If you aren't continuing from the last chapter you may need to use the full path to your main folder.

Our next step will be to download the data you want to work with onto Bluewaves.
First we need to create a **script** to make this happen.
A script is a detailed set of instructions for the computer to follow.
Your script is going to be a series of instructions to download files and put them in particular folders so you can access them.
We are going to use the program `nano`, which is built into the system, to write this script.

* Type `nano download_data.sh`.

Again, we have a command followed by an argument (in this case the name of the file we are creating).
Notice how the file ends in `.sh`.
Just like you have files that end in `.xls` or `.doc` this tells the computer the file is a shell script.

Commands in your shell script and just like commands on the command line.
For each file you want to download type the following (separated by spaces) on a single line:

* `wget`
* The ftp link (make sure it is not preceed by http://)
* `-P`
* the scientific name (as one word without spaces)

* Repeat with the second ftp link for the sample if it has one.

* Repeat this process for each sample you want to download.

* Exit `nano` using `ctrl-x` and save your changes.

There are many people doing work on the cluster so we need take advantage of the computing power of the cluster
so that each student in the class can run their analysis on a separate computer.
When you log in to the cluster you are on the "head node".
From here you can submit jobs to run analyses on the other nodes attached to the cluster.
In order to communicate information about your job to the cluster you need to add the following lines to the beginning of your download script.

```
#!/bin/bash
#SBATCH --job-name="download"
#SBATCH --time=150:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1   # number of nodes
#SBATCH --ntasks-per-node=36   # processor core(s) per node
#SBATCH --mail-user="rsschwartz@uri.edu" 

cd $SLURM_SUBMIT_DIR

```

* Replace the email address on the second-to-last line with yours.
* Quit nano and save.
* Submit your job as `sbatch download_data.sh`.
* Monitor the progress of your download by listing the contents of your `data` folder and the contents of the folders inside it.
